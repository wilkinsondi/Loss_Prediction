{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set locations\n",
    "train_data_location = 'C:/Users/darre/Documents/Algospark/HSBC/loan-default-prediction/train_v2.csv'\n",
    "test_data_location = 'C:/Users/darre/Documents/Algospark/HSBC/loan-default-prediction/test_v2.csv'\n",
    "save_data_location = 'C:/Users/darre/Documents/Algospark/HSBC/loan-default-prediction/preds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2718: DtypeWarning: Columns (135,204,274,417) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2718: DtypeWarning: Columns (417) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "## Import data\n",
    "train_data = pd.read_csv(train_data_location)\n",
    "test_data = pd.read_csv(test_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f770</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>f776</th>\n",
       "      <th>f777</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>13699</td>\n",
       "      <td>7201.0</td>\n",
       "      <td>4949.0</td>\n",
       "      <td>126.75</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.7873</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>10</td>\n",
       "      <td>0.782776</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>84645</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>123.52</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>-0.6787</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>83607</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>127.76</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>10</td>\n",
       "      <td>0.439874</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>82642</td>\n",
       "      <td>7542.0</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>132.94</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.29</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>9</td>\n",
       "      <td>0.502749</td>\n",
       "      <td>2900</td>\n",
       "      <td>4</td>\n",
       "      <td>79124</td>\n",
       "      <td>89.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>122.72</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   f1  f2        f3    f4  f5     f6      f7      f8      f9  ...   f770  \\\n",
       "0   1  126  10  0.686842  1100   3  13699  7201.0  4949.0  126.75  ...      5   \n",
       "1   2  121  10  0.782776  1100   3  84645   240.0  1625.0  123.52  ...      6   \n",
       "2   3  126  10  0.500080  1100   3  83607  1800.0  1527.0  127.76  ...     13   \n",
       "3   4  134  10  0.439874  1100   3  82642  7542.0  1730.0  132.94  ...      4   \n",
       "4   5  109   9  0.502749  2900   4  79124    89.0   491.0  122.72  ...     26   \n",
       "\n",
       "   f771  f772  f773    f774    f775  f776  f777  f778  loss  \n",
       "0  2.14 -1.54  1.18  0.1833  0.7873     1     0     5     0  \n",
       "1  0.54 -0.24  0.13  0.1926 -0.6787     1     0     5     0  \n",
       "2  2.89 -1.73  1.04  0.2521  0.7258     1     0     5     0  \n",
       "3  1.29 -0.89  0.66  0.2498  0.7119     1     0     5     0  \n",
       "4  6.11 -3.82  2.51  0.2282 -0.5399     0     0     5     0  \n",
       "\n",
       "[5 rows x 771 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data\n",
    "train_data.head()\n",
    "### placeholder: data analytics goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 771)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of train data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210944, 770)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of test data (note test data does not have \"loss\" column)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 742)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data clean. Remove type=object columns and columns with no info..\n",
    "for i in train_data.select_dtypes(include=['object']).columns:\n",
    "    train_data.drop(labels=i, axis=1, inplace=True)\n",
    "    \n",
    "for i in train_data.columns:\n",
    "    if len(set(train_data[i]))==1:\n",
    "        train_data.drop(labels=[i], axis=1, inplace=True)\n",
    "        \n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210944, 741)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align test dataframe to same columns as in training\n",
    "unique=set(train_data.columns).intersection(set(test_data.columns))\n",
    "test_data2= test_data[test_data.columns.intersection(unique)]\n",
    "test_data2.shape\n",
    "#Should be 1 column less due to missing \"loss\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a default metric, ie loss = 1 or 0 on training set\n",
    "train_data['default'] = train_data.loss.apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 743)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set gaps to median and remove any NA values.\n",
    "cleaned_data = train_data.fillna(train_data.median())\n",
    "cleaned_data.dropna(axis=0)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84376, 741)\n",
      "(21095, 741)\n",
      "(84376, 1)\n",
      "(21095, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split training data into train & validate sets\n",
    "## Leave loss value in for later split of dataframe\n",
    "\n",
    "features = cleaned_data.drop(axis=1, labels=['default','id'])\n",
    "targets = pd.DataFrame(cleaned_data['default'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, targets, test_size = 0.2, random_state = 73)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84376L, 740L)\n",
      "(21095L, 740L)\n",
      "(84376L,)\n",
      "(21095L,)\n"
     ]
    }
   ],
   "source": [
    "# Remove loss value, normalize the  data & convert to arrays\n",
    "X_train_scaled = X_train.drop(axis=1, labels='loss')\n",
    "X_val_scaled = X_val.drop(axis=1, labels='loss')\n",
    "\n",
    "X_train_scaled = sc.fit_transform(X_train_scaled)\n",
    "X_val_scaled = sc.transform(X_val_scaled)\n",
    "y_train_scaled = np.array(y_train).reshape((-1, ))\n",
    "y_val_scaled = np.array(y_val).reshape((-1, ))\n",
    "print(X_train_scaled.shape)\n",
    "print(X_val_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(y_val_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random fprest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 20, max_depth=20, min_samples_split=5, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Find optimal F1 for a grid of cutoffs\n",
    "def bestF1(obs,pred):\n",
    "    best = 0\n",
    "    bestcut = 0\n",
    "    for cutoff in np.arange(0.01,0.99,0.01):\n",
    "        tmp = f1_score(obs,pd.Series(pred > cutoff).apply(lambda x: 1 if x else 0))\n",
    "        if tmp > best:\n",
    "            best = tmp\n",
    "            bestcut = cutoff\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit classifier model\n",
    "model1 =rf_classifier.fit(X_train_scaled,y_train_scaled)\n",
    "# Placeholder for cross validation <here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.927586043425\n",
      "0.894473684211\n",
      "0.99029440777\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics for training data default prediction\n",
    "y_train_preds = model1.predict_proba(X_train_scaled)[:,1]\n",
    "y_train_include = np.where(y_train_preds<0.5,0,1)\n",
    "accuracy_train = accuracy_score(y_train_scaled,y_train_include)\n",
    "F1_train = bestF1(y_train_scaled,y_train_preds)\n",
    "AUC_train = roc_auc_score(y_train_scaled,y_train_preds)\n",
    "print accuracy_train\n",
    "print F1_train\n",
    "print AUC_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909267598957\n",
      "0.218712514994\n",
      "0.645469832862\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics for validation data default prediction \n",
    "y_val_preds = model1.predict_proba(X_val_scaled)[:,1]\n",
    "y_val_include = np.where(y_val_preds<0.5,0,1)\n",
    "accuracy_val = accuracy_score(y_val_scaled,y_val_include)\n",
    "F1_val = bestF1(y_val_scaled,y_val_preds)\n",
    "AUC_val = roc_auc_score(y_val_scaled,y_val_preds)\n",
    "print accuracy_val\n",
    "print F1_val\n",
    "print AUC_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random forest for loss value prediction\n",
    "rf_predictor = RandomForestRegressor(n_estimators=200,max_depth=20,min_samples_split=2,random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "### Subset the training set based on instances classified as default\n",
    "X_train_loss = X_train #take the dataframe with the loss information\n",
    "X_train_loss['pred_loss']=y_train_include #Add predicted loss for subsetting\n",
    "X_train_loss_zero = X_train_loss[X_train_loss['pred_loss']==0]\n",
    "\n",
    "#Subset, transform and calculate based on predicted default category\n",
    "X_train_loss_positive = X_train_loss[X_train_loss['pred_loss']>0]\n",
    "y_train_loss_positive = pd.DataFrame(X_train_loss_positive['loss'])\n",
    "y_train_loss_positive = np.array(y_train_loss_positive).reshape((-1, ))\n",
    "X_train_default = X_train_loss_positive.drop(axis=1, labels=['pred_loss','loss']) ## Remove the columns for losses and predicted losses\n",
    "X_train_loss_positive_scaled = sc.fit_transform(X_train_default) #Scale\n",
    "\n",
    "#Same for the validation set\n",
    "X_val_loss = X_val #take the dataframe with the loss information\n",
    "X_val_loss['pred_loss']=y_val_include #Add predicted loss for subsetting\n",
    "X_val_loss_zero = X_val_loss[X_val_loss['pred_loss']==0]\n",
    "X_val_loss_positive = X_val_loss[X_val_loss['pred_loss']>0]\n",
    "y_val_loss_positive = pd.DataFrame(X_val_loss_positive['loss'])\n",
    "y_val_loss_positive = np.array(y_val_loss_positive).reshape((-1, ))\n",
    "X_val_default = X_val_loss_positive.drop(axis=1, labels=['pred_loss','loss']) ## Remove the columns for losses and predicted losses\n",
    "X_val_loss_positive_scaled = sc.fit_transform(X_val_default) #Scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Default prediction model\n",
    "model2 =rf_predictor.fit(X_train_loss_positive_scaled,np.log(y_train_loss_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54376106702\n"
     ]
    }
   ],
   "source": [
    "# Calculate training MAE on predicted defaults for training data\n",
    "preds_loss_train = np.e**model2.predict(X_train_loss_positive_scaled) ##convert back from logs\n",
    "MAE_train = np.mean(np.abs(preds_loss_train-y_train_loss_positive))\n",
    "print MAE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731210898373\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE of predicted loss across all samples for training data\n",
    "X_train_loss_positive['pred_loss'] = preds_loss_train ### Convert the pred loss value into the predicted\n",
    "X_train_with_loss = pd.concat([X_train_loss_positive,X_train_loss_zero])\n",
    "MAE_train_all = np.mean(np.abs(X_train_with_loss['pred_loss'] - X_train_with_loss['loss'] ))\n",
    "print MAE_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58428108768\n"
     ]
    }
   ],
   "source": [
    "# Calculate validation MAE on predicted defaults for validation data\n",
    "preds_loss_val = np.e**model2.predict(X_val_loss_positive_scaled) ##convert back from logs\n",
    "MAE_val = np.mean(np.abs(preds_loss_val - y_val_loss_positive))\n",
    "print MAE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742917412225\n"
     ]
    }
   ],
   "source": [
    "#Calculate MAE of predicted loss across all samples for validation data\n",
    "X_val_loss_positive['pred_loss'] = preds_loss_val ### Convert the pred loss value into the predicted\n",
    "X_val_with_loss = pd.concat([X_val_loss_positive,X_val_loss_zero])\n",
    "MAE_val_all = np.mean(np.abs(X_val_with_loss['pred_loss'] - X_val_with_loss['loss'] ))\n",
    "print MAE_val_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210944, 740)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Clean the test data\n",
    "test_cleaned = test_data2.fillna(cleaned_data.median())\n",
    "test_cleaned.fillna(0) #captures any NA's from median calc\n",
    "test_id = test_cleaned['id']\n",
    "test_cleaned = test_cleaned.drop(axis=1, labels=['id'])\n",
    "test_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210944L, 740L)\n"
     ]
    }
   ],
   "source": [
    "#Transform test data\n",
    "X_test_scaled = sc.fit_transform(test_cleaned)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get default predictions\n",
    "y_test_preds = model1.predict_proba(X_test_scaled)[:,1]\n",
    "y_test_include = np.where(y_test_preds<0.5,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare test data for loss predictions\n",
    "X_test_loss = test_cleaned #take the dataframe with the loss information\n",
    "X_test_loss['pred_loss']=y_test_include #Add predicted loss for subsetting\n",
    "X_test_loss['id']=test_id\n",
    "X_test_loss_zero = X_test_loss[X_test_loss['pred_loss']==0]\n",
    "\n",
    "#Subset, transform and calculate based on predicted loss category\n",
    "X_test_loss_positive = X_test_loss[X_test_loss['pred_loss']>0]\n",
    "X_test_loss_positive= X_test_loss_positive.drop(axis=1, labels='pred_loss')\n",
    "test_id2 =X_test_loss_positive['id']\n",
    "X_test_loss_positive= X_test_loss_positive.drop(axis=1, labels='id')\n",
    "X_test_loss_positive_scaled = sc.fit_transform(X_test_loss_positive) #Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate predicted losses on defaults\n",
    "preds_loss_test = np.e**model2.predict(X_test_loss_positive_scaled) ##convert back from logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate values across cases and write to csv\n",
    "X_test_loss_positive['pred_loss'] = preds_loss_test ### Convert the pred loss value into the predicted\n",
    "X_test_loss_positive['id'] = test_id2\n",
    "test_data_with_predictions = pd.concat([X_test_loss_positive,X_test_loss_zero])\n",
    "test_data_with_predictions.rename(columns={'pred_loss':'loss'}, inplace=True)\n",
    "test_data_with_predictions[['id','loss']].to_csv(save_data_location,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
