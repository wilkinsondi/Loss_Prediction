{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "train_data = pd.read_csv('C:/Users/darre/Documents/Algospark/HSBC/loan-default-prediction/train_v2.csv')\n",
    "test_data = pd.read_csv('C:/Users/darre/Documents/Algospark/HSBC/loan-default-prediction/test_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f770</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>f776</th>\n",
       "      <th>f777</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>13699</td>\n",
       "      <td>7201.0</td>\n",
       "      <td>4949.0</td>\n",
       "      <td>126.75</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.7873</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>10</td>\n",
       "      <td>0.782776</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>84645</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>123.52</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>-0.6787</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>83607</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>127.76</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>10</td>\n",
       "      <td>0.439874</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>82642</td>\n",
       "      <td>7542.0</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>132.94</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.29</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>9</td>\n",
       "      <td>0.502749</td>\n",
       "      <td>2900</td>\n",
       "      <td>4</td>\n",
       "      <td>79124</td>\n",
       "      <td>89.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>122.72</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   f1  f2        f3    f4  f5     f6      f7      f8      f9  ...   f770  \\\n",
       "0   1  126  10  0.686842  1100   3  13699  7201.0  4949.0  126.75  ...      5   \n",
       "1   2  121  10  0.782776  1100   3  84645   240.0  1625.0  123.52  ...      6   \n",
       "2   3  126  10  0.500080  1100   3  83607  1800.0  1527.0  127.76  ...     13   \n",
       "3   4  134  10  0.439874  1100   3  82642  7542.0  1730.0  132.94  ...      4   \n",
       "4   5  109   9  0.502749  2900   4  79124    89.0   491.0  122.72  ...     26   \n",
       "\n",
       "   f771  f772  f773    f774    f775  f776  f777  f778  loss  \n",
       "0  2.14 -1.54  1.18  0.1833  0.7873     1     0     5     0  \n",
       "1  0.54 -0.24  0.13  0.1926 -0.6787     1     0     5     0  \n",
       "2  2.89 -1.73  1.04  0.2521  0.7258     1     0     5     0  \n",
       "3  1.29 -0.89  0.66  0.2498  0.7119     1     0     5     0  \n",
       "4  6.11 -3.82  2.51  0.2282 -0.5399     0     0     5     0  \n",
       "\n",
       "[5 rows x 771 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 771)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of train data\n",
    "train_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210944, 770)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of test data\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 742)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Remove type=object columns and columns with no info..\n",
    "for i in train_data.select_dtypes(include=['object']).columns:\n",
    "    train_data.drop(labels=i, axis=1, inplace=True)\n",
    "    \n",
    "for i in train_data.columns:\n",
    "    if len(set(train_data[i]))==1:\n",
    "        train_data.drop(labels=[i], axis=1, inplace=True)\n",
    "        \n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210944, 741)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align test dataframe to same columns\n",
    "unique=set(train_data.columns).intersection(set(test_data.columns))\n",
    "test_data2= test_data[test_data.columns.intersection(unique)]\n",
    "test_data2.shape\n",
    "#Should be 1 column less due to missing loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default metric for loss: 1 or 0 on training set\n",
    "train_data['default'] = train_data.loss.apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105471, 743)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the training data set by setting nulls to median and excluding any remaining NA values.\n",
    "cleaned_data = train_data.fillna(train_data.median())\n",
    "cleaned_data.dropna(axis=0)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84376, 741)\n",
      "(21095, 741)\n",
      "(84376, 1)\n",
      "(21095, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split training data into train & validate sets\n",
    "## Leave loss value in for later split of dataframe\n",
    "\n",
    "features = cleaned_data.drop(axis=1, labels=['default','id'])\n",
    "targets = pd.DataFrame(cleaned_data['default'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, targets, test_size = 0.2, random_state = 73)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84376L, 740L)\n",
      "(21095L, 740L)\n",
      "(84376L,)\n",
      "(21095L,)\n"
     ]
    }
   ],
   "source": [
    "# Remove loss value, normalize the  data & convert to arrays\n",
    "\n",
    "X_train_scaled = X_train.drop(axis=1, labels='loss')\n",
    "X_val_scaled = X_val.drop(axis=1, labels='loss')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train_scaled)\n",
    "X_val_scaled = sc.transform(X_val_scaled)\n",
    "y_train_scaled = np.array(y_train).reshape((-1, ))\n",
    "y_val_scaled = np.array(y_val).reshape((-1, ))\n",
    "print(X_train_scaled.shape)\n",
    "print(X_val_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(y_val_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 20, max_depth=20, min_samples_split=5, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Find optimal F1 for a grid of cutoffs\n",
    "def bestF1(obs,pred):\n",
    "    best = 0\n",
    "    bestcut = 0\n",
    "    for cutoff in np.arange(0.01,0.99,0.01):\n",
    "        tmp = f1_score(obs,pd.Series(pred > cutoff).apply(lambda x: 1 if x else 0))\n",
    "        if tmp > best:\n",
    "            best = tmp\n",
    "            bestcut = cutoff\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for cross validation <here>\n",
    "\n",
    "#Fit model\n",
    "model1 =rf_classifier.fit(X_train_scaled,y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.927586043425\n",
      "0.894473684211\n",
      "0.99029440777\n"
     ]
    }
   ],
   "source": [
    "# Train metrics for loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_train_preds = model1.predict_proba(X_train_scaled)[:,1]\n",
    "y_train_include = np.where(y_train_preds<0.5,0,1)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train_scaled,y_train_include)\n",
    "F1_train = bestF1(y_train_scaled,y_train_preds)\n",
    "AUC_train = roc_auc_score(y_train_scaled,y_train_preds)\n",
    "print accuracy_train\n",
    "print F1_train\n",
    "print AUC_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909267598957\n",
      "0.218712514994\n",
      "0.645469832862\n"
     ]
    }
   ],
   "source": [
    "# Validation metrics for loss \n",
    "y_val_preds = model1.predict_proba(X_val_scaled)[:,1]\n",
    "y_val_include = np.where(y_val_preds<0.5,0,1)\n",
    "\n",
    "accuracy_val = accuracy_score(y_val_scaled,y_val_include)\n",
    "F1_val = bestF1(y_val_scaled,y_val_preds)\n",
    "AUC_val = roc_auc_score(y_val_scaled,y_val_preds)\n",
    "\n",
    "print accuracy_val\n",
    "print F1_val\n",
    "print AUC_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### For cases predicted as deafult, calculate loss using rf regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_predictor = RandomForestRegressor(n_estimators=200,max_depth=20,min_samples_split=2,random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "### Subset the training set based on classified prediction of a loss\n",
    "X_train_loss = X_train #take the dataframe with the loss information\n",
    "X_train_loss['pred_loss']=y_train_include #Add predicted loss for subsetting\n",
    "X_train_loss_zero = X_train_loss[X_train_loss['pred_loss']==0]\n",
    "\n",
    "#Subset, transform and calculate based on predicted loss category\n",
    "X_train_loss_positive = X_train_loss[X_train_loss['pred_loss']>0]\n",
    "y_train_loss_positive = pd.DataFrame(X_train_loss_positive['loss'])\n",
    "y_train_loss_positive = np.array(y_train_loss_positive).reshape((-1, ))\n",
    "X_train_default = X_train_loss_positive.drop(axis=1, labels=['pred_loss','loss']) ## Remove the columns for losses and predicted losses\n",
    "X_train_loss_positive_scaled = sc.fit_transform(X_train_default) #Scale\n",
    "\n",
    "#Same for the validation set\n",
    "X_val_loss = X_val #take the dataframe with the loss information\n",
    "X_val_loss['pred_loss']=y_val_include #Add predicted loss for subsetting\n",
    "X_val_loss_zero = X_val_loss[X_val_loss['pred_loss']==0]\n",
    "\n",
    "X_val_loss_positive = X_val_loss[X_val_loss['pred_loss']>0]\n",
    "y_val_loss_positive = pd.DataFrame(X_val_loss_positive['loss'])\n",
    "y_val_loss_positive = np.array(y_val_loss_positive).reshape((-1, ))\n",
    "X_val_default = X_val_loss_positive.drop(axis=1, labels=['pred_loss','loss']) ## Remove the columns for losses and predicted losses\n",
    "X_val_loss_positive_scaled = sc.fit_transform(X_val_default) #Scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Default prediction model\n",
    "model2 =rf_predictor.fit(X_train_loss_positive_scaled,np.log(y_train_loss_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54376106702\n"
     ]
    }
   ],
   "source": [
    "# Calculate training MAE on subset of predicted losses only\n",
    "preds_loss_train = np.e**model2.predict(X_train_loss_positive_scaled) ##convert back from logs\n",
    "MAE_train = np.mean(np.abs(preds_loss_train-y_train_loss_positive))\n",
    "print MAE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731210898373\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE across all samples for training data\n",
    "X_train_loss_positive['pred_loss'] = preds_loss_train ### Convert the pred loss value into the predicted\n",
    "X_train_with_loss = pd.concat([X_train_loss_positive,X_train_loss_zero])\n",
    "MAE_train_all = np.mean(np.abs(X_train_with_loss['pred_loss'] - X_train_with_loss['loss'] ))\n",
    "print MAE_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58428108768\n"
     ]
    }
   ],
   "source": [
    "# Calculate validation MAE on subset of predicted losses only\n",
    "preds_loss_val = np.e**model2.predict(X_val_loss_positive_scaled) ##convert back from logs\n",
    "MAE_val = np.mean(np.abs(preds_loss_val - y_val_loss_positive))\n",
    "print MAE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darre\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742917412225\n"
     ]
    }
   ],
   "source": [
    "#Calculate MAE across all samples for validation data\n",
    "X_val_loss_positive['pred_loss'] = preds_loss_val ### Convert the pred loss value into the predicted\n",
    "X_val_with_loss = pd.concat([X_val_loss_positive,X_val_loss_zero])\n",
    "MAE_val_all = np.mean(np.abs(X_val_with_loss['pred_loss'] - X_val_with_loss['loss'] ))\n",
    "print MAE_val_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210944, 740)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Use the model with the test set\n",
    "# Clean the data set by setting nulls to median and convert NA's to zeroes (as a placeholder)\n",
    "test_cleaned = test_data2.fillna(cleaned_data.median())\n",
    "test_cleaned.fillna(0) #captures any NA's from median calc\n",
    "test_id = test_cleaned['id']\n",
    "test_cleaned = test_cleaned.drop(axis=1, labels=['id'])\n",
    "test_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210944L, 740L)\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = sc.fit_transform(test_cleaned)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get default predictions\n",
    "y_test_preds = model1.predict_proba(X_test_scaled)[:,1]\n",
    "y_test_include = np.where(y_test_preds<0.5,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_loss = test_cleaned #take the dataframe with the loss information\n",
    "X_test_loss['pred_loss']=y_test_include #Add predicted loss for subsetting\n",
    "X_test_loss['id']=test_id\n",
    "X_test_loss_zero = X_test_loss[X_test_loss['pred_loss']==0]\n",
    "\n",
    "#Subset, transform and calculate based on predicted loss category\n",
    "X_test_loss_positive = X_test_loss[X_test_loss['pred_loss']>0]\n",
    "X_test_loss_positive= X_test_loss_positive.drop(axis=1, labels='pred_loss')\n",
    "test_id2 =X_test_loss_positive['id']\n",
    "X_test_loss_positive= X_test_loss_positive.drop(axis=1, labels='id')\n",
    "X_test_loss_positive_scaled = sc.fit_transform(X_test_loss_positive) #Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted losses on defaults\n",
    "preds_loss_test = np.e**model2.predict(X_test_loss_positive_scaled) ##convert back from logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_loss_positive['pred_loss'] = preds_loss_test ### Convert the pred loss value into the predicted\n",
    "X_test_loss_positive['id'] = test_id2\n",
    "test_data_with_predictions = pd.concat([X_test_loss_positive,X_test_loss_zero])\n",
    "test_data_with_predictions.rename(columns={'pred_loss':'loss'}, inplace=True)\n",
    "test_data_with_predictions[['id','loss']].to_csv(\"C:/Users/darre/Documents/Algospark/HSBC/loan-default-prediction/preds.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
